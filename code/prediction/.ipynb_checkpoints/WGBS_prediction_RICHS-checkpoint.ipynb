{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from common import commons\n",
    "home = commons.home\n",
    "sys.path.append('/home/ec2-user/anaconda3/lib/python3.6/site-packages')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models import ModelSelection as MS\n",
    "from models import Ensemble as es\n",
    "from models import xgbooster\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_score,accuracy_score,f1_score,roc_curve,roc_auc_score,precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import clone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from importlib import reload\n",
    "from log import Logger\n",
    "from models import deep_network_estimator as dne\n",
    "from models import Ensemble_hyperopt as eh\n",
    "from hyperopt import fmin,tpe,hp, STATUS_OK,Trials\n",
    "from hyperopt_models import parallel_ensemble as pe\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import prediction_commons\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_curves_cv(probs,label,methods,types='roc_curve'):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(types)\n",
    "    plt.axis([0,1,0,1])\n",
    "    lw = 2\n",
    "    colors = ['r','b','g','k','c','m','y']\n",
    "    for method,color in zip(methods,colors[:len(methods)]):\n",
    "        if types == 'precision_recall_curve':       \n",
    "            precision,recall,threshold = precision_recall_curve(label,probs[method])\n",
    "            plt.plot(recall,precision,color,linewidth=2,label=method)\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "        if types == 'roc_curve':\n",
    "            fpr,tpr, threshold = roc_curve(label,probs[method])\n",
    "            plt.plot(fpr,tpr,color,linewidth=2,label=method)\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('Recall')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "def get_estimators(methods,params,train_x,train_label):\n",
    "    ensemble = eh.Ensemble(methods,params)\n",
    "    ensemble.fit(train_x,train_label,sample_weight=sample_weight_train,max_iter=100)\n",
    "    return ensemble\n",
    "#-----------------------------------------------------------------------------\n",
    "def plot_curves(estimators,test_x,label,types='roc_curve'):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(types)\n",
    "    plt.axis([0,1,0,1])\n",
    "    lw = 2\n",
    "    colors = ['r','b','g','k','c','m','y']\n",
    "    for color,estimator in zip(colors[:len(estimators)],estimators):\n",
    "        name = type(estimator).__name__\n",
    "        probs = np.array(estimator.predict_proba(test_x))[:,1]\n",
    "        if types == 'precision_recall_curve':       \n",
    "            precision,recall,threshold = precision_recall_curve(label,probs)\n",
    "            plt.plot(recall,precision,color,linewidth=2,label=name)\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "        if types == 'roc_curve':\n",
    "            fpr,tpr, threshold = roc_curve(label,probs)\n",
    "            plt.plot(fpr,tpr,color,linewidth=2,label=name)\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('Recall')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.legend(loc='best')\n",
    "#-----------------------------------------------------------------------------\n",
    "def learn_curve(model,train_x,train_label,cv=3,scoring='neg_log_loss'):\n",
    "    model_c = clone(model)\n",
    "    N,train_score,test_score = learning_curve(model_c, \n",
    "                                            train_x,train_label,cv=cv,train_sizes=np.linspace(0.5,1,5),\n",
    "                                            scoring=scoring)\n",
    "    \n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.title('{}'.format(type(model).__name__))\n",
    "    plt.plot(N,np.mean(train_score,1),color='blue', label='training score')\n",
    "    plt.plot(N,np.mean(test_score,1),color='red',label='validation score')\n",
    "    plt.xlabel('training sample')\n",
    "    plt.ylabel(scoring)\n",
    "    plt.legend(loc=0)\n",
    "    plt.show()\n",
    "#-----------------------------------------------------------------------------    \n",
    "def error_analysis(estimator,test_x,label,types=['confusion_matrix']):\n",
    "    print('Error analysis of ',type(estimator).__name__)\n",
    "    predict = estimator.predict(test_x)\n",
    "    probs = np.array(estimator.predict_proba(test_x))[:,1]\n",
    "    class_num = len(label.unique())\n",
    "    if 'confusion_matrix' in types:\n",
    "        conf_mat = confusion_matrix(label,predict)\n",
    "        row_sums = conf_mat.sum(axis=1,keepdims=True)\n",
    "        norm_conf_mat = conf_mat/row_sums\n",
    "        np.fill_diagonal(norm_conf_mat,0)\n",
    "        plt.matshow(norm_conf_mat,cmap=plt.cm.gray)        \n",
    "    if 'precision_recall_curve' in types and class_num<=2:\n",
    "        precision,recall,threshold = precision_recall_curve(label,probs)\n",
    "        plot_curve(recall,precision,type(estimator).__name__,'precision_recall_curve')\n",
    "    if 'roc_curve' in types and class_num<=2:\n",
    "        fpr,tpr, threshold = roc_curve(label,probs)\n",
    "        plot_curve(fpr,tpr,type(estimator).__name__,'roc_curve')    \n",
    "    plt.show()\n",
    "#---------------------------------------------------------------------------    \n",
    "def plot_curve(score1,score2,label,types):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(types)\n",
    "    plt.plot(score1,score2,linewidth=2,label=label)\n",
    "    plt.plot([0,1],[1,1],'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    lw = 2\n",
    "    if types == 'precision_recall_curve':        \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "    if types == 'roc_curve':\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "#--------------------------------------------------------------------------\n",
    "def scores(y,predicts,pred_probs,average='macro'):\n",
    "    class_num = len(y.unique())\n",
    "    score_map = {}\n",
    "\n",
    "    if class_num <=2:\n",
    "        recall = recall_score(y,predicts)      \n",
    "        precision = precision_score(y,predicts)      \n",
    "        accuracy = accuracy_score(y,predicts)\n",
    "        f1 = f1_score(y,predicts)\n",
    "        auc = roc_auc_score(y,pred_probs)\n",
    "        score_map['auc'] = auc\n",
    "    else:\n",
    "        recall = recall_score(y,predicts,average=average)      \n",
    "        precision = precision_score(y,predicts,average=average)      \n",
    "        accuracy = accuracy_score(y,predicts)\n",
    "        f1 = f1_score(y,predicts,average=average)\n",
    "        \n",
    "    score_map['recall'] = recall\n",
    "    score_map['precision'] = precision\n",
    "    score_map['accuracy'] = accuracy\n",
    "    score_map['f1'] = f1\n",
    "    return score_map\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "def cross_val_ensemble(x,y,sample_weight,methods,params,fold=10, hyperopt=True,up_sampling=False):\n",
    "    skfolds = StratifiedKFold(n_splits=10,random_state=43)\n",
    "    results = []\n",
    "    model_combine_scores_cv = []\n",
    "    model_scores_cv = []\n",
    "    best_params_cv = []\n",
    "    cv_columns = ['label','ensemble']+methods\n",
    "    pred_probs_cv = pd.DataFrame(columns=cv_columns)\n",
    "    predicts_cv = pd.DataFrame(columns=cv_columns)\n",
    "    if not hyperopt:\n",
    "        search_methods = get_train_models(models=methods)\n",
    "    class_num = len(y.unique())\n",
    "    for train_index,test_index in skfolds.split(x,y):\n",
    "        train_fold = x.ix[train_index,:]\n",
    "        train_label = y[train_index]\n",
    "        sample_weight_train = sample_weight[train_index]\n",
    "        test_fold = x.ix[test_index,:]\n",
    "        test_label = y[test_index]\n",
    "        sample_weight_test = sample_weight[test_index]\n",
    "        if up_sampling:\n",
    "            train_fold,train_label,sample_weight_train = upsampling(train_fold,train_label,sample_weight_train,fold=9)\n",
    "            test_fold,test_label,sample_weight_test = downsampling(test_fold,test_label,sample_weight_test)\n",
    "        if not hyperopt:            \n",
    "            for param_l in params.values():\n",
    "                param = param_l[0]\n",
    "                if 'sample_weight' in param:\n",
    "                    param['sample_weight'] = [sample_weight_train,]\n",
    "            ensemble = es.Ensemble(methods=search_methods,params=params)\n",
    "            ensemble.fit(train_fold,train_label)            \n",
    "        else:\n",
    "            ensemble = eh.Ensemble(methods,params)\n",
    "            ensemble.fit(train_fold,train_label,sample_weight=sample_weight_train,max_iter=100)\n",
    "        score = ensemble.score(test_fold,test_label)\n",
    "        results.extend([score])\n",
    "        model_prob = ensemble.model_probs()\n",
    "        model_preds = ensemble.model_predicts() \n",
    "        ensemble_prob = ensemble.predict_proba(test_fold)\n",
    "        ensemble_pred = ensemble.predict(test_fold)\n",
    "        \n",
    "        temp_df = pd.DataFrame(columns=pred_probs_cv.columns)\n",
    "        for method,prob in model_prob.items():\n",
    "            temp_df[method] = prob[:,1]\n",
    "            temp_df[method] = temp_df[method].astype('f')\n",
    "        temp_df['ensemble'] = ensemble_prob[:,1]\n",
    "        temp_df['ensemble'] = temp_df['ensemble'].astype('f')\n",
    "        temp_df['label'] = test_label\n",
    "        temp_df['label'] = temp_df['label'].astype('i8')\n",
    "        pred_probs_cv = pred_probs_cv.append(temp_df,ignore_index=True)\n",
    "        \n",
    "        \n",
    "        temp_df = pd.DataFrame(columns=predicts_cv.columns)\n",
    "        for method,pred in model_preds.items():\n",
    "            temp_df[method] = pred\n",
    "            temp_df[method] = temp_df[method].astype('i8')\n",
    "        temp_df['ensemble'] = ensemble_pred\n",
    "        temp_df['ensemble'] = temp_df['ensemble'].astype('i8')\n",
    "        temp_df['label'] = test_label\n",
    "        temp_df['label'] = temp_df['label'].astype('i8')\n",
    "        predicts_cv = predicts_cv.append(temp_df,ignore_index=True)\n",
    "        \n",
    "        comb_results = methods_combination_results(methods,model_prob,test_label)\n",
    "        model_combine_scores_cv.extend([comb_results])\n",
    "        model_score = ensemble.get_model_scores()\n",
    "        model_scores_cv.extend([model_score.copy()])\n",
    "        best_params = ensemble.best_params()\n",
    "        best_params_cv.extend([best_params.copy()])\n",
    "        all_estimators = list(ensemble.best_estimators_.values())\n",
    "        all_estimators.extend([ensemble])\n",
    "        plot_curves(all_estimators,test_fold,test_label,types='roc_curve')\n",
    "        plot_curves(all_estimators,test_fold,test_label,types='precision_recall_curve')\n",
    "        del ensemble\n",
    "    if class_num == 2:\n",
    "        result_df = pd.DataFrame(results,columns=['logloss','f1','recall','precision','auc_score'])\n",
    "    else:\n",
    "        result_df = pd.DataFrame(results,columns=['logloss','f1','recall','precision'])\n",
    "    return result_df,model_combine_scores_cv,model_scores_cv,best_params_cv,pred_probs_cv,predicts_cv\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "def soft_voting(model_probs):\n",
    "    pred_probs = reduce(lambda x,y: np.add(x,y), model_probs.values())/len(model_probs)\n",
    "    print(pred_probs)\n",
    "    predicts = np.argmax(pred_probs,axis=1)\n",
    "    return pred_probs,predicts    \n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def get_train_models(models=['LogisticRegression','RandomForestClassifier','SVC','MLPClassifier','xgbooster','tensor_DNN']):\n",
    "    methods = []\n",
    "    l = LogisticRegression()\n",
    "    rf = RandomForestClassifier()\n",
    "    svc = SVC()\n",
    "    xg = xgbooster.xgbooster()\n",
    "    mlp = MLPClassifier()\n",
    "    dnn = dne.tensor_DNN()\n",
    "    lsvc = LinearSVC()\n",
    "    if 'LogisticRegression' in models:\n",
    "        methods.extend([l])\n",
    "    if 'RandomForestClassifier' in models:\n",
    "        methods.extend([rf])\n",
    "    if 'SVC' in models:\n",
    "        methods.extend([svc])\n",
    "    if 'MLPClassifier' in models:\n",
    "        methods.extend([mlp])\n",
    "    if 'xgbooster' in models:\n",
    "        methods.extend([xg])\n",
    "    if 'tensor_DNN' in models:\n",
    "        methods.extend([dnn])\n",
    "    if 'LinearSVC' in models:\n",
    "        methods.extend([lsvc])\n",
    "    return methods\n",
    " #------------------------------------------------------------------------------\n",
    "\n",
    "def get_hyperopt_params(methods=['LogisticRegression','RandomForestClassifier','LinearSVC','SVC','xgbooster','tensor_DNN','MLPClassifier'],wtf_lo=1,wtf_hi=1):\n",
    "    weight_factor = hp.uniform('weight_factor',wtf_lo,wtf_hi)\n",
    "    weight_factor = hp.uniform('weight_factor',wtf_lo,wtf_hi)\n",
    "    params={}\n",
    "    l_param = {'C': hp.uniform('C',0.05,20),'weight_factor':weight_factor}\n",
    "    rf_param = {'n_estimators':100+hp.randint('n_estimators',900),'max_depth':5+hp.randint('max_depth',20), 'min_samples_split': 5+hp.randint('min_samples_split',15),'min_samples_leaf': 2+hp.randint('min_samples_leaf',4),'weight_factor':weight_factor}\n",
    "    svc_param = {'C': hp.uniform('C',0.005,1),'gamma': hp.uniform('gamma',0.001,1),'probability':hp.choice('probability',[True]),'weight_factor':weight_factor}\n",
    "    xgb_param = {'learning_rate':hp.choice('learning_rate',[0.1]),'max_depth': 5+hp.randint('max_depth',15),'n_estimators':500+hp.randint('n_estimators',2000),'reg_lambda': hp.uniform('reg_lambda',20,100),'gamma': hp.uniform('gamma',0.01,10),'weight_factor':weight_factor}\n",
    "    dnn_param = {'batch_normalization': hp.choice('batch_normalization',[True]),\n",
    "                 'l2_reg': hp.uniform('l2_reg',0.001,5),                            \n",
    "                 'drop_out':hp.uniform('drop_out',0.1,0.8),\n",
    "                 'weight_factor':weight_factor,\n",
    "                 'steps':200+hp.randint('steps',1000),\n",
    "                 'batch_size':hp.choice('batch_size',[30]),\n",
    "                 'scoring':hp.choice('scoring',['precision']),\n",
    "                 }\n",
    "    mlp_param = {'alpha':hp.uniform('alpha',0.001,5),'max_iter':2000+hp.randint('max_iter',1000)}\n",
    "    lsvc_param = {'C': hp.uniform('C',0.1,10),'weight_factor':weight_factor} \n",
    "    if 'LogisticRegression' in methods:\n",
    "        params['LogisticRegression'] = l_param\n",
    "    if 'RandomForestClassifier' in methods:\n",
    "        params['RandomForestClassifier'] = rf_param\n",
    "    if 'SVC' in methods:\n",
    "        params['SVC'] = svc_param\n",
    "    if 'xgbooster' in methods:\n",
    "        params['xgbooster'] = xgb_param\n",
    "    if 'tensor_DNN' in methods:\n",
    "        params['tensor_DNN'] = dnn_param\n",
    "    if 'MLPClassifier' in methods:\n",
    "        params['MLPClassifier'] = mlp_param\n",
    "    if 'LinearSVC' in methods:\n",
    "        params['LinearSVC'] = lsvc_param\n",
    "    return params    \n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def methods_combination_results(methods,model_probs,test_label):\n",
    "    n = len(methods)\n",
    "    results = {}\n",
    "    for i in range(1,n+1):\n",
    "        iterator = itertools.combinations(methods,i)\n",
    "        for combination in iterator:\n",
    "            key = reduce(lambda x,y: x+'-'+y,combination)\n",
    "            print(key)\n",
    "            test_model_probs = {method:prob for method,prob in model_probs.items() if method in combination}\n",
    "            pred_probs,pred = soft_voting(test_model_probs)\n",
    "            test_score = scores(test_label,pred,pred_probs[:,1])\n",
    "            results[key] = test_score.copy()\n",
    "    return results\n",
    "#---------------------------------------------------------------------\n",
    "def upsampling(train_x,train_label,sample_weights_train,fold=9):\n",
    "    trainx = train_x.copy()\n",
    "    trainx['label'] = train_label\n",
    "    trainx['weight'] = sample_weights_train\n",
    "    up_samples = commons.upSampling(trainx[trainx['label']==1],10)\n",
    "    trainx = trainx.append(up_samples,ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    train_label = trainx['label']\n",
    "    sample_weights_train = trainx['weight']\n",
    "    trainx = trainx.drop(['label','weight'],axis=1)\n",
    "    return trainx,train_label,sample_weights_train\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def downsampling(x,y,sample_weights):\n",
    "    x = x.copy()\n",
    "    x['label'] = y\n",
    "    x['weight'] = sample_weights\n",
    "    pos = x[x['label']==1]\n",
    "    negs = x[x['label']==0].sample(pos.shape[0])\n",
    "    alls = pos.append(negs,ignore_index=True)\n",
    "    label = alls['label']\n",
    "    weights = alls['weight']\n",
    "    return alls.drop(['label','weight'],axis=1),label,weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = 'RICHS'\n",
    "start = str(prediction_commons.tss_start)\n",
    "end = str(prediction_commons.tss_end)\n",
    "all_features = home+'data/WGBS/all_features_'+str(start)+'_'+str(end)\n",
    "model_path = home+'data/'+dataset+'/prediction_model.pkl'\n",
    "pred_probs = home+'data/'+dataset+'/pred_probs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##features selecetd by traditional methods\n",
    "\n",
    "up_sampling = True;\n",
    "if up_sampling:\n",
    "    wtf_lo = 0.05 if dataset==\"RICHS\" else 0.2\n",
    "    wtf_hi = 0.1 if dataset==\"RICHS\" else 0.3\n",
    "else:\n",
    "    wtf_lo = 1.0/3 if dataset==\"RICHS\" else 1 \n",
    "    wtf_hi = 0.5 if dataset==\"RICHS\" else 1.5\n",
    "    \n",
    "log_dir = home+'logs/'\n",
    "logger = Logger.Logger(log_dir,False).get_logger()\n",
    "with pd.HDFStore(home+'data/'+dataset+'/selected_features','r') as h5s:\n",
    "    train_x =h5s['train_x'] \n",
    "    train_label = h5s['train_label'] \n",
    "    test_x = h5s['test_x'] \n",
    "    test_label = h5s['test_label']\n",
    "    sample_weights_train = h5s['sample_weights_train'] \n",
    "    sample_weights_test = h5s['sample_weights_test']\n",
    "total_x = pd.concat([train_x,test_x],ignore_index=True)\n",
    "total_label = pd.concat([train_label,test_label],ignore_index=True)\n",
    "total_sample_weights = pd.concat([sample_weights_train,sample_weights_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06257936422045392\n",
      "0.05879581453749219\n",
      "0.07446193091400137\n",
      "0.07531317478043249\n",
      "0.0580592985529368\n",
      "0.05220659547174434\n",
      "0.05222590656120484\n",
      "0.07125663055138862\n",
      "0.09447215558794711\n",
      "0.060305602431045\n",
      "0.06973615408508438\n",
      "0.08572508981333915\n",
      "0.08973691066311691\n",
      "0.0779892177111967\n",
      "0.050218119565793376\n",
      "0.0559822706186447\n",
      "0.07952621699883139\n",
      "0.09607983267143169\n",
      "0.09999442406937173\n",
      "0.07711578583412541\n",
      "0.06489659304880065\n",
      "0.0645467328441619\n",
      "0.05073387171553087\n",
      "0.05152675520431073\n",
      "0.05555100589334018\n",
      "0.06707133258011827\n",
      "0.050083986598185394\n",
      "0.0827244863029014\n",
      "0.061916976355611296\n",
      "0.05526225267017396\n",
      "0.05047679836772707\n",
      "0.0710001970520055\n",
      "0.0555832821505718\n",
      "0.05489122536374158\n",
      "0.058523202112770005\n",
      "0.061448833888146225\n",
      "0.07388990482943383\n",
      "0.06750925968919554\n",
      "0.05473922620365963\n",
      "0.05858530298674522\n",
      "0.06429340275315211\n",
      "0.05384748899345283\n",
      "0.0606475390299928\n",
      "0.058369392727964806\n",
      "0.05843374027108165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06891906481175919\n",
      "0.08792503344821744\n",
      "0.07271993268868002\n",
      "0.05170716262478313\n",
      "0.09311119739663937\n",
      "0.08117787450142148\n",
      "0.052706772665412016\n",
      "0.0766595040617642\n",
      "0.05015365026741342\n",
      "0.06647886112343648\n",
      "0.0629141232927447\n",
      "0.052961135033580437\n",
      "0.057405044534633326\n",
      "0.056760764556775034\n",
      "0.09729055064742716\n",
      "0.09817734533638449\n",
      "0.08436252644488806\n",
      "0.091569317520474\n",
      "0.08927521191208217\n",
      "0.09924662523616501\n",
      "0.09597897233951092\n",
      "0.08442773826114028\n",
      "0.060101228975827856\n",
      "0.06325277984896285\n",
      "0.07885470421645893\n",
      "0.07466426627045694\n",
      "0.08186544238768495\n",
      "0.05048226387017892\n",
      "0.07104278716091196\n",
      "0.06583927550163066\n",
      "0.06909889620022562\n",
      "0.0530046654870562\n",
      "0.08784411626113564\n",
      "0.05698799113451064\n",
      "0.06807003619171338\n",
      "0.06003419815616871\n",
      "0.07235557165157885\n",
      "0.053993815900455835\n",
      "0.06381284828748301\n",
      "0.09420107598848734\n",
      "0.07566263753203925\n",
      "0.061524726362597086\n",
      "0.06998834771866755\n",
      "0.06587985303168989\n",
      "0.057003826820794516\n",
      "0.09696763595192667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05938479304021108\n",
      "0.05950539247868514\n",
      "0.05227871001922284\n",
      "0.09127338385676546\n",
      "0.07978603440014195\n",
      "0.08649404406544178\n",
      "0.05581713885373458\n",
      "0.061966766356862164\n",
      "0.05147425360590114\n",
      "{'C': 18.6068436312954, 'weight_factor': 0.09729055064742716}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/git/EnsembleCpG/data/RICHS/estimators.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######model training\n",
    "methods = ['LogisticRegression','xgbooster']\n",
    "params = get_hyperopt_params(methods,wtf_lo=wtf_lo,wtf_hi=wtf_hi)\n",
    "ensemble_hyopt = eh.Ensemble(methods,params)\n",
    "ensemble_hyopt.fit(total_x,total_label,sample_weight=total_sample_weights,max_iter=100)\n",
    "joblib.dump(ensemble_hyopt,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_features = total_x.columns\n",
    "with pd.HDFStore(all_features,'r') as h5s:\n",
    "    wgbs_all_data = h5s['all_features']\n",
    "wgbs_data = wgbs_all_data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression,0.09729055064742716\n"
     ]
    }
   ],
   "source": [
    "ensemble_hyopt = joblib.load(model_path)\n",
    "for method,best_estimator in ensemble_hyopt.best_estimators_.items():\n",
    "    best_estimator.fit(total_x,total_label,total_sample_weights)\n",
    "pred = ensemble_hyopt.predict(wgbs_data)\n",
    "pred_prob = ensemble_hyopt.predict_proba(wgbs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_prob = pd.DataFrame(pred_prob,columns=['negative','positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/py3.6/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "target_sites = pred_prob.sort_values(['positive'],ascending=False).query('positive > 0.5')\n",
    "target_sites_coordinate = wgbs_all_data.ix[target_sites.index,['chr','coordinate']]\n",
    "target_sites = target_sites.join(target_sites_coordinate)\n",
    "with pd.HDFStore(pred_probs,'w') as h5s:\n",
    "    h5s[start+'_'+end] = target_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>chr</th>\n",
       "      <th>coordinate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>845070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5534</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5721</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>859325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5701</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5702</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5703</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5704</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5719</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5718</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5717</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5716</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5714</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5713</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5712</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>860704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>0.413415</td>\n",
       "      <td>0.586585</td>\n",
       "      <td>1</td>\n",
       "      <td>228594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>0.413507</td>\n",
       "      <td>0.586493</td>\n",
       "      <td>1</td>\n",
       "      <td>228524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>0.414229</td>\n",
       "      <td>0.585771</td>\n",
       "      <td>1</td>\n",
       "      <td>228492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>0.416285</td>\n",
       "      <td>0.583715</td>\n",
       "      <td>1</td>\n",
       "      <td>228537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>0.417306</td>\n",
       "      <td>0.582694</td>\n",
       "      <td>1</td>\n",
       "      <td>228518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>0.418993</td>\n",
       "      <td>0.581007</td>\n",
       "      <td>1</td>\n",
       "      <td>678118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4629</th>\n",
       "      <td>0.420022</td>\n",
       "      <td>0.579978</td>\n",
       "      <td>1</td>\n",
       "      <td>837108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4628</th>\n",
       "      <td>0.421438</td>\n",
       "      <td>0.578562</td>\n",
       "      <td>1</td>\n",
       "      <td>837095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>0.424062</td>\n",
       "      <td>0.575938</td>\n",
       "      <td>1</td>\n",
       "      <td>837138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4626</th>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.574800</td>\n",
       "      <td>1</td>\n",
       "      <td>837027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4627</th>\n",
       "      <td>0.428714</td>\n",
       "      <td>0.571286</td>\n",
       "      <td>1</td>\n",
       "      <td>837041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413</th>\n",
       "      <td>0.443436</td>\n",
       "      <td>0.556564</td>\n",
       "      <td>1</td>\n",
       "      <td>676946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>0.445437</td>\n",
       "      <td>0.554563</td>\n",
       "      <td>1</td>\n",
       "      <td>676985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>0.459006</td>\n",
       "      <td>0.540994</td>\n",
       "      <td>1</td>\n",
       "      <td>665713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>0.473833</td>\n",
       "      <td>0.526167</td>\n",
       "      <td>1</td>\n",
       "      <td>665632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3545</th>\n",
       "      <td>0.477543</td>\n",
       "      <td>0.522457</td>\n",
       "      <td>1</td>\n",
       "      <td>766204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8775</th>\n",
       "      <td>0.479950</td>\n",
       "      <td>0.520050</td>\n",
       "      <td>1</td>\n",
       "      <td>931326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8772</th>\n",
       "      <td>0.481576</td>\n",
       "      <td>0.518424</td>\n",
       "      <td>1</td>\n",
       "      <td>931235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8773</th>\n",
       "      <td>0.483939</td>\n",
       "      <td>0.516061</td>\n",
       "      <td>1</td>\n",
       "      <td>931262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>0.484004</td>\n",
       "      <td>0.515996</td>\n",
       "      <td>1</td>\n",
       "      <td>668082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3546</th>\n",
       "      <td>0.484051</td>\n",
       "      <td>0.515949</td>\n",
       "      <td>1</td>\n",
       "      <td>766284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8776</th>\n",
       "      <td>0.484659</td>\n",
       "      <td>0.515341</td>\n",
       "      <td>1</td>\n",
       "      <td>931335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8774</th>\n",
       "      <td>0.485939</td>\n",
       "      <td>0.514061</td>\n",
       "      <td>1</td>\n",
       "      <td>931312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8770</th>\n",
       "      <td>0.486087</td>\n",
       "      <td>0.513913</td>\n",
       "      <td>1</td>\n",
       "      <td>931228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771</th>\n",
       "      <td>0.488170</td>\n",
       "      <td>0.511830</td>\n",
       "      <td>1</td>\n",
       "      <td>931231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8777</th>\n",
       "      <td>0.490649</td>\n",
       "      <td>0.509351</td>\n",
       "      <td>1</td>\n",
       "      <td>931337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548</th>\n",
       "      <td>0.490666</td>\n",
       "      <td>0.509334</td>\n",
       "      <td>1</td>\n",
       "      <td>766327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8778</th>\n",
       "      <td>0.492490</td>\n",
       "      <td>0.507510</td>\n",
       "      <td>1</td>\n",
       "      <td>931360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3549</th>\n",
       "      <td>0.493446</td>\n",
       "      <td>0.506554</td>\n",
       "      <td>1</td>\n",
       "      <td>766376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>0.497024</td>\n",
       "      <td>0.502976</td>\n",
       "      <td>1</td>\n",
       "      <td>766291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5331 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      negative  positive  chr  coordinate\n",
       "5000  0.000000  1.000000    1      845070\n",
       "5528  0.000000  1.000000    1      859360\n",
       "5700  0.000000  1.000000    1      860612\n",
       "5534  0.000000  1.000000    1      859386\n",
       "5533  0.000000  1.000000    1      859381\n",
       "5532  0.000000  1.000000    1      859378\n",
       "5531  0.000000  1.000000    1      859374\n",
       "5530  0.000000  1.000000    1      859369\n",
       "5529  0.000000  1.000000    1      859367\n",
       "5527  0.000000  1.000000    1      859352\n",
       "5721  0.000000  1.000000    1      860743\n",
       "5526  0.000000  1.000000    1      859350\n",
       "5525  0.000000  1.000000    1      859340\n",
       "5524  0.000000  1.000000    1      859338\n",
       "5523  0.000000  1.000000    1      859336\n",
       "5522  0.000000  1.000000    1      859332\n",
       "5521  0.000000  1.000000    1      859328\n",
       "5520  0.000000  1.000000    1      859325\n",
       "5701  0.000000  1.000000    1      860620\n",
       "5702  0.000000  1.000000    1      860628\n",
       "5703  0.000000  1.000000    1      860639\n",
       "5704  0.000000  1.000000    1      860652\n",
       "5719  0.000000  1.000000    1      860735\n",
       "5718  0.000000  1.000000    1      860732\n",
       "5717  0.000000  1.000000    1      860727\n",
       "5716  0.000000  1.000000    1      860715\n",
       "5715  0.000000  1.000000    1      860712\n",
       "5714  0.000000  1.000000    1      860710\n",
       "5713  0.000000  1.000000    1      860708\n",
       "5712  0.000000  1.000000    1      860704\n",
       "...        ...       ...  ...         ...\n",
       "1896  0.413415  0.586585    1      228594\n",
       "1892  0.413507  0.586493    1      228524\n",
       "1887  0.414229  0.585771    1      228492\n",
       "1893  0.416285  0.583715    1      228537\n",
       "1889  0.417306  0.582694    1      228518\n",
       "2422  0.418993  0.581007    1      678118\n",
       "4629  0.420022  0.579978    1      837108\n",
       "4628  0.421438  0.578562    1      837095\n",
       "4630  0.424062  0.575938    1      837138\n",
       "4626  0.425200  0.574800    1      837027\n",
       "4627  0.428714  0.571286    1      837041\n",
       "2413  0.443436  0.556564    1      676946\n",
       "2414  0.445437  0.554563    1      676985\n",
       "2280  0.459006  0.540994    1      665713\n",
       "2279  0.473833  0.526167    1      665632\n",
       "3545  0.477543  0.522457    1      766204\n",
       "8775  0.479950  0.520050    1      931326\n",
       "8772  0.481576  0.518424    1      931235\n",
       "8773  0.483939  0.516061    1      931262\n",
       "2302  0.484004  0.515996    1      668082\n",
       "3546  0.484051  0.515949    1      766284\n",
       "8776  0.484659  0.515341    1      931335\n",
       "8774  0.485939  0.514061    1      931312\n",
       "8770  0.486087  0.513913    1      931228\n",
       "8771  0.488170  0.511830    1      931231\n",
       "8777  0.490649  0.509351    1      931337\n",
       "3548  0.490666  0.509334    1      766327\n",
       "8778  0.492490  0.507510    1      931360\n",
       "3549  0.493446  0.506554    1      766376\n",
       "3547  0.497024  0.502976    1      766291\n",
       "\n",
       "[5331 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
